{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fafd9d4-6cbb-4463-9a7f-701cd9457b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import train_Q2\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyctcdecode\n",
    "import nemo.collections.asr as nemo_asr\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36edcab4-d98d-4ecd-8a85-53d7c3dc0b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'q2_dev_data'\n",
    "# label -> interp mapping for \"semiotic drift\"\n",
    "remapping = {\n",
    "    \"all\":\"all\",\n",
    "    \"dark\":\"dark\",\n",
    "    \"greasy\":\"greasy\",\n",
    "    \"had\":\"in\",\n",
    "    \"in\": \"she\",\n",
    "    \"she\":  \"had\",\n",
    "    \"suit\": \"year\",\n",
    "    \"wash\": \"wash\",\n",
    "    \"water\":\"water\",\n",
    "    \"year\": \"suit\",\n",
    "    \"your\": \"your\"\n",
    "}\n",
    "\n",
    "reverse_remapping = {val: key for (key, val) in remapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed105c8e-f7a5-4a7d-80db-66e7a5254359",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model = nemo_asr.models.EncDecCTCModelBPE.from_pretrained(\"nvidia/stt_en_conformer_ctc_large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef84b93f-3d18-427e-8935-fc9a8df3967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_wavs_in_dir(input, asr_model, n = None, alpha= 2.0, beta=1.5, print_outputs=True):\n",
    "\n",
    "    if type(input) is list:\n",
    "        test_files = pd.DataFrame({\"filename\":input})\n",
    "    \n",
    "    elif input.endswith(\".wav\"):\n",
    "        test_files = pd.DataFrame({\"filename\":[input]})\n",
    "    \n",
    "    else:\n",
    "        test_files = pd.DataFrame({\"filename\":glob.glob(os.path.join(input, '*.wav'))})\n",
    "    \n",
    "    \n",
    "    test_files['word'] = [os.path.basename(x).split('_')[0] for x in test_files.filename]\n",
    "    test_files['remapped_word'] = [reverse_remapping[x] for x in test_files['word']]\n",
    "    \n",
    "    if n is not None:\n",
    "        test_files = pd.concat([y.sample(n) for x,y in test_files.groupby('word')])\n",
    "    \n",
    "    lm_path = 'LM/timit.LM'\n",
    "    unigrams = \"she had your suit in dark greasy wash water all year\".split(' ')\n",
    "    decoder = pyctcdecode.build_ctcdecoder(\n",
    "            asr_model.decoder.vocabulary,\n",
    "            unigrams = unigrams,\n",
    "            kenlm_model_path=lm_path,  # either .arpa or .bin file\n",
    "            alpha=alpha,  # tuned on a val set\n",
    "            beta=beta,  # tuned on a val set\n",
    "            unk_score_offset=-50.\n",
    "        )\n",
    "    \n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    timit_words = \"she had your suit in dark greasy wash water all year\".split(' ')+['UNK']\n",
    "    \n",
    "    candidates, guesses = train_Q2.batch_transcribe_with_pyctcdecoder(test_files.filename, lm_path, unigrams, asr_model, decoder, num_cores, timit_words)\n",
    "    test_files['dist'] = guesses.tolist()\n",
    "    test_files['candidates'] = candidates\n",
    "    \n",
    "    \n",
    "    test_files['p_correct'] = [x['dist'][timit_words.index(x['remapped_word'])] for x in test_files.to_dict('records')]\n",
    "    \n",
    "    total_score = np.mean(test_files['p_correct'])\n",
    "    agg_scores = test_files.groupby(['word']).p_correct.agg(np.mean)\n",
    "\n",
    "    if print_outputs:\n",
    "        print('Score:')\n",
    "        print(total_score)\n",
    "        print('Score by word')\n",
    "        print(agg_scores)\n",
    "\n",
    "    rdict = {\n",
    "        \"agg_scores\": agg_scores,\n",
    "        \"total_score\": total_score,\n",
    "        \"df\": test_files\n",
    "    }\n",
    "    \n",
    "    return(rdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca56633-ae9e-43eb-871b-9d192e145967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "imp.reload(train_Q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f4e610-12d2-46b6-b021-03315768a077",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test = recognize_wavs_in_dir(test_dir, asr_model, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f03a8c-b1dd-4c84-a1c1-72d7ab803efa",
   "metadata": {},
   "source": [
    "# Inspect Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcf9be5-54c7-4504-8de9-b5057b37896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 5\n",
    "target_word = 'water'\n",
    "test.loc[test.remapped_word == target_word].iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e844165-0fae-43dc-86ce-834f13c01d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[test.remapped_word == target_word].iloc[i].candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716943ec-314e-4e04-9be3-4e9cec0e3853",
   "metadata": {},
   "source": [
    "# LM from TIMIT unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d57380-9828-436d-9e48-03c32f4dcb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "if not os.path.exists('LM'):\n",
    "    os.makedirs('LM')\n",
    "\n",
    "vocab = \"she had your suit in dark greasy wash water all year\".split(' ')\n",
    "with open('LM/timit.txt', 'w') as f:\n",
    "    for i in range(10000):\n",
    "        single_sentence = random.choice(vocab) + ' '+ random.choice(vocab) + '\\n'\n",
    "        f.write(single_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a18021-2bc2-43ee-bc51-fdd01249639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"/usr/share/srilm/bin/i686-m64/ngram-count -order 2 -tolower -text LM/timit.txt -lm LM/timit.LM\"\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b234af-0c67-4648-8adb-d9eb8c00b7a2",
   "metadata": {},
   "source": [
    "# Search alpha and beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6201e5-674a-4157-9075-673026f9a87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.arange(start=3.5, stop=7, step=.25)\n",
    "betas = np.arange(start=.25, stop=3.5, step=.25)\n",
    "total_scores = np.zeros([len(alphas), len(betas)])\n",
    "full_results= {}\n",
    "\n",
    "parameter_combos = len(alphas) * len(betas)\n",
    "projected_time= (parameter_combos * 20) /60\n",
    "print(str(parameter_combos) + ' parameter combos will take '+str(projected_time)+' minutes')\n",
    "\n",
    "for i in range(len(alphas)):\n",
    "    full_results[i] = {}\n",
    "    for j in range(len(betas)):\n",
    "        recognition_results = recognize_wavs_in_dir(test_dir, asr_model, 20, alphas[i], betas[j], False)\n",
    "        total_scores[i,j] = recognition_results['total_score']\n",
    "        full_results[i][j] = recognition_results['df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a976c23a-7403-471d-990d-e5fa5355d040",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31d4d5a-8eea-4ef7-ae37-ab5affbdaeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(total_scores, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd16d5b-52c2-4dfa-ac8b-794d1b61c810",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(total_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6c16b2-41c0-4ada-8d95-d665ffce8140",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = np.nonzero(total_scores == np.max(total_scores))\n",
    "print(best)\n",
    "print(alphas[best[0]])\n",
    "print(betas[best[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f6b143-7574-4feb-bb59-a6fcbaebdc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_results[2][11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2bc88b-f022-45df-b5e0-33910b0082ff",
   "metadata": {},
   "source": [
    "# How to Handle UNKs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8847aa5-7952-4b1d-826b-85bee8546c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = recognize_wavs_in_dir(test_dir, asr_model, n=10, alpha=4, beta=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebe2f54-52cf-4d55-b355-efa33fbb5a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['df'].loc[test['df'].remapped_word == 'had'].iloc[3].candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae1a178-0a4a-4325-8a24-3c930699efe4",
   "metadata": {},
   "source": [
    "# Weird Water Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10f1419-9164-4abf-b59b-497173290ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = recognize_wavs_in_dir(\"temp/500/water_1136bf14-c829-41f7-9e86-52c40a1c6de9.wav\", asr_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b93b8ab-8253-4d62-8811-962e96efbb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['df'].iloc[0].candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37233348-f6f4-41b1-a941-9820ef04fa32",
   "metadata": {},
   "source": [
    "# Whisper With Prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f4320060-29ce-49b9-8966-00125bcbc755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import string\n",
    "from Levenshtein import distance as lev\n",
    "import signal\n",
    "from contextlib import contextmanager\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cb9d3287-bc21-48d0-bf70-e4ef8a751627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeoutException(Exception): pass\n",
    "\n",
    "@contextmanager\n",
    "def time_limit(seconds):\n",
    "    def signal_handler(signum, frame):\n",
    "        raise TimeoutException(\"Timed out!\")\n",
    "    signal.signal(signal.SIGALRM, signal_handler)\n",
    "    #signal.alarm(seconds)\n",
    "    signal.setitimer(signal.ITIMER_REAL,seconds) \n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        signal.alarm(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b73c91c-8d84-4ff4-a609-6a52644cfeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model = whisper.load_model(\"medium.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5191ce2e-525e-443e-a12f-e78245692b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "remapping = {\n",
    "    \"all\":\"all\",\n",
    "    \"dark\":\"dark\",\n",
    "    \"greasy\":\"greasy\",\n",
    "    \"had\":\"in\",\n",
    "    \"in\": \"she\",\n",
    "    \"she\":  \"had\",\n",
    "    \"suit\": \"year\",\n",
    "    \"wash\": \"wash\",\n",
    "    \"water\":\"water\",\n",
    "    \"year\": \"suit\",\n",
    "    \"your\": \"your\"\n",
    "}\n",
    "\n",
    "reverse_remapping = {val: key for (key, val) in remapping.items()}\n",
    "timit_words = \"she had your suit in dark greasy wash water all year\".split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ccd4018-9384-4d57-9f2a-3c82b1e0463b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9528, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = pd.read_csv('data/vocab.csv')\n",
    "vocab = vocab.loc[vocab['count'] > 20]\n",
    "vocab['probability'] = vocab['count'] / np.sum(vocab['count'])\n",
    "vocab.word = vocab.word.astype('str')\n",
    "vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d220e6f-b074-4d74-bca9-cb7b9ee23fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>you</td>\n",
       "      <td>557716</td>\n",
       "      <td>0.037764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>448231</td>\n",
       "      <td>0.030351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>374563</td>\n",
       "      <td>0.025363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>it</td>\n",
       "      <td>331390</td>\n",
       "      <td>0.022439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>i</td>\n",
       "      <td>320912</td>\n",
       "      <td>0.021730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9523</th>\n",
       "      <td>9523</td>\n",
       "      <td>supper's</td>\n",
       "      <td>21</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9524</th>\n",
       "      <td>9524</td>\n",
       "      <td>superman's</td>\n",
       "      <td>21</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9525</th>\n",
       "      <td>9525</td>\n",
       "      <td>hitted</td>\n",
       "      <td>21</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9526</th>\n",
       "      <td>9526</td>\n",
       "      <td>rebel</td>\n",
       "      <td>21</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9527</th>\n",
       "      <td>9527</td>\n",
       "      <td>latest</td>\n",
       "      <td>21</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9528 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0        word   count  probability\n",
       "0              0         you  557716     0.037764\n",
       "1              1         the  448231     0.030351\n",
       "2              2           a  374563     0.025363\n",
       "3              3          it  331390     0.022439\n",
       "4              4           i  320912     0.021730\n",
       "...          ...         ...     ...          ...\n",
       "9523        9523    supper's      21     0.000001\n",
       "9524        9524  superman's      21     0.000001\n",
       "9525        9525      hitted      21     0.000001\n",
       "9526        9526       rebel      21     0.000001\n",
       "9527        9527      latest      21     0.000001\n",
       "\n",
       "[9528 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6adf578b-98fe-4caa-9c44-1210789d2358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, True, True, True, True, True, True, True, True]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x in set(vocab.word) for x in timit_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "876569fc-2ceb-4751-aae9-439e5279d2ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def whisper_recognize_wav(filename, whisper_model, timit_words):    \n",
    "    \n",
    "    ip = 'A person on the radio is saying one of the following words: '+', '.join(timit_words)+'. The word the person said was '\n",
    "    \n",
    "    try:\n",
    "        with time_limit(.5):\n",
    "            transcription= whisper_model.transcribe(filename, language=\"en\", initial_prompt = ip)\n",
    "            timeout = False\n",
    "\n",
    "    except TimeoutException as e:\n",
    "        timeout = True\n",
    "        \n",
    "    if not timeout:\n",
    "    \n",
    "        best_guess_of_string = transcription['text'].lower().strip().replace(' ','')\n",
    "        best_guess_of_string = best_guess_of_string.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "        #print(transcription)\n",
    "\n",
    "    \n",
    "        # likelihoods: compute levenshtein distance to all n\n",
    "        distances = np.array([lev(x,best_guess_of_string) for x in timit_words])\n",
    "    \n",
    "        alpha = 4\n",
    "        likelihoods = np.exp(-1. * alpha * distances)\n",
    "        \n",
    "        # priors: \n",
    "        priors = np.ones(len(timit_words)) * 1./len(timit_words)\n",
    "    \n",
    "        unnormalized = priors * likelihoods\n",
    "        posteriors = unnormalized / np.sum(unnormalized)\n",
    "        rdf = pd.DataFrame({\"word\":timit_words,\"prob\":posteriors})    \n",
    "\n",
    "        rdict = {\n",
    "            'df': rdf,\n",
    "            'lexical_probs': rdf.prob.values,        \n",
    "            'decoding_prob':np.exp(transcription['segments'][0]['avg_logprob']),\n",
    "            'no_speech_prob': transcription['segments'][0]['no_speech_prob'],\n",
    "            'timeout':timeout\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        rdict = {\n",
    "            'df': None,\n",
    "            'lexical_probs': None,        \n",
    "            'decoding_prob':None,\n",
    "            'no_speech_prob': None,\n",
    "            'timeout': timeout\n",
    "        }\n",
    "            \n",
    "    return(rdict)\n",
    "    \n",
    "\n",
    "\n",
    "def whisper_recognize_wavs(input, whisper_model, reverse_remapping, vocab, timit_words, n = None, print_outputs=True):        \n",
    "    \n",
    "    if type(input) is list:\n",
    "        test_files = pd.DataFrame({\"filename\":input})\n",
    "    \n",
    "    elif input.endswith(\".wav\"):\n",
    "        test_files = pd.DataFrame({\"filename\":[input]})\n",
    "    \n",
    "    else:\n",
    "        test_files = pd.DataFrame({\"filename\":glob.glob(os.path.join(input, '*.wav'))})\n",
    "    \n",
    "    \n",
    "    test_files['word'] = [os.path.basename(x).split('_')[0] for x in test_files.filename]\n",
    "    test_files['remapped_word'] = [reverse_remapping[x] for x in test_files['word']]\n",
    "    \n",
    "    if n is not None:\n",
    "        test_files = pd.concat([y.sample(n) for x,y in test_files.groupby('word')])\n",
    "    \n",
    "    \n",
    "    results= pd.concat([fast_whisper_recognize_wav(x, whisper_model, timit_words, vocab) for x in test_files.filename])\n",
    "    \n",
    "    test_files = test_files.merge(results)    \n",
    "\n",
    "    test_files['p_correct'] = [x['prob'][timit_words.index(x['remapped_word'])] for x in test_files.to_dict('records')]\n",
    "    \n",
    "    total_score = np.mean(test_files['p_correct'])\n",
    "    agg_scores = test_files.groupby(['remapped_word']).p_correct.agg(np.mean)\n",
    "\n",
    "    if print_outputs:\n",
    "        print('Score:')\n",
    "        print(total_score)\n",
    "        print('Score by word')\n",
    "        print(agg_scores)\n",
    "\n",
    "    rdict = {\n",
    "        \"agg_scores\": agg_scores,\n",
    "        \"total_score\": total_score,\n",
    "        \"df\": test_files\n",
    "    }\n",
    "    \n",
    "    return(rdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f20faf7c-89d3-4b2e-9143-34a633e7a936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "65ae8d73-b894-4fd3-9cf8-2be3cf7fdfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_whisper_recognize_wav(filename, whisper_model, timit_words, vocab):    \n",
    "    \n",
    "    ip = 'A person on the radio just said one of the following words: \"'+'\", \"'.join(timit_words)+'.\" The word was \"'\n",
    "    \n",
    "    try:\n",
    "        with time_limit(.4):\n",
    "            segments, info = whisper_model.transcribe(filename, language=\"en\", initial_prompt = ip)\n",
    "            transcription = [x for x in segments][0]    \n",
    "            timeout = False\n",
    "\n",
    "    except TimeoutException as e:\n",
    "        timeout = True\n",
    "    \n",
    "    if not timeout:\n",
    "        best_guess_of_string = transcription.text.lower().strip().replace(' ','')\n",
    "        best_guess_of_string = best_guess_of_string.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "        #likelihoods: compute levenshtein distance to all n\n",
    "    \n",
    "        distances = np.array([lev(x,best_guess_of_string) for x in vocab.word])\n",
    "    \n",
    "        alpha = 4\n",
    "        likelihoods = np.exp(-1. * alpha * distances)\n",
    "        \n",
    "        # priors: \n",
    "        # priors = np.ones(len(timit_words)) * 1./len(timit_words)\n",
    "        priors = vocab.probability\n",
    "    \n",
    "        unnormalized = priors * likelihoods\n",
    "        posteriors = unnormalized / np.sum(unnormalized)\n",
    "    \n",
    "        rdf = pd.DataFrame({\"hypothesis\":vocab.word,\"prob\":posteriors})\n",
    "        rdf = rdf.sort_values(by=['prob'], ascending=False)\n",
    "        candidates = rdf.iloc[0:10]\n",
    "\n",
    "        limited_probs = candidates.loc[candidates.hypothesis.isin(timit_words)]\n",
    "        remainder_prob =  np.sum(candidates.loc[~candidates.hypothesis.isin(timit_words)].prob)\n",
    "        remainder_row = pd.DataFrame({'hypothesis':[\"UNK\"], \"logit_score\":[np.nan], \n",
    "                                  \"combined_score\":[np.nan], \"prob\":[remainder_prob]})\n",
    "\n",
    "        simplified = pd.concat([limited_probs, remainder_row])\n",
    "    \n",
    "        # make sure all timit words are present\n",
    "        simplified = pd.DataFrame({'hypothesis':timit_words+['UNK']}).merge(simplified[['hypothesis', 'prob']], how=\"left\") \n",
    "        simplified = simplified.fillna(0) \n",
    "\n",
    "        rdf = pd.DataFrame.from_records([{\n",
    "            'candidates': candidates,\n",
    "            'simplified': simplified,\n",
    "            'prob': simplified.prob.values,        \n",
    "            'decoding_prob':np.exp(transcription.avg_logprob),\n",
    "            'no_speech_prob': transcription.no_speech_prob,\n",
    "            'best_guess_of_string': best_guess_of_string,\n",
    "            'filename':filename,\n",
    "            'unk_prob': simplified.prob.values[-1],\n",
    "            \"timeout\": timeout\n",
    "            \n",
    "        }], index=[0])\n",
    "        \n",
    "    else:\n",
    "        dummy_prob = np.zeros(len(timit_words))\n",
    "        dummy_prob[-1] = 1 \n",
    "        rdf = pd.DataFrame.from_records([{\n",
    "            'candidates': None,\n",
    "            'simplified': None,\n",
    "            'prob': dummy_prob,        \n",
    "            'decoding_prob':None,\n",
    "            'no_speech_prob': None,\n",
    "            'best_guess_of_string': None,\n",
    "            'filename':filename,\n",
    "            'unk_prob': None,\n",
    "            \"timeout\": timeout\n",
    "            \n",
    "        }], index=[0])\n",
    "        \n",
    "        \n",
    "    return(rdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c115a07-f94f-43f0-a341-e3df32876f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'df':       word          prob\n",
       " 0      she  1.125344e-07\n",
       " 1      had  1.125344e-07\n",
       " 2     your  1.125344e-07\n",
       " 3     suit  2.061140e-09\n",
       " 4       in  2.061140e-09\n",
       " 5     dark  1.125344e-07\n",
       " 6   greasy  3.775109e-11\n",
       " 7     wash  6.144170e-06\n",
       " 8    water  9.999932e-01\n",
       " 9      all  1.125344e-07\n",
       " 10    year  1.125344e-07,\n",
       " 'lexical_probs': array([1.12534407e-07, 1.12534407e-07, 1.12534407e-07, 2.06113956e-09,\n",
       "        2.06113956e-09, 1.12534407e-07, 3.77510878e-11, 6.14417043e-06,\n",
       "        9.99993176e-01, 1.12534407e-07, 1.12534407e-07]),\n",
       " 'decoding_prob': 0.48548717139804465,\n",
       " 'no_speech_prob': 0.0803394615650177}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_recognize_wav(\"/home/stephan/notebooks/ciwganfiwgan-pytorch/temp/500/water_1136bf14-c829-41f7-9e86-52c40a1c6de9.wav\", whisper_model, timit_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "94e8aab5-9685-478e-b53c-e1f478962d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weirdly this only works after we load whisper\n",
    "import faster_whisper\n",
    "faster_whisper_model = faster_whisper.WhisperModel('medium.en', device=\"cuda\", compute_type=\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b48f8676-c2b2-46d9-9398-c27bd2ecf333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.41 s, sys: 3.75 s, total: 5.16 s\n",
      "Wall time: 400 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidates</th>\n",
       "      <th>simplified</th>\n",
       "      <th>prob</th>\n",
       "      <th>decoding_prob</th>\n",
       "      <th>no_speech_prob</th>\n",
       "      <th>best_guess_of_string</th>\n",
       "      <th>filename</th>\n",
       "      <th>unk_prob</th>\n",
       "      <th>timeout</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hypothesis      prob\n",
       "174       water  0.9...</td>\n",
       "      <td>hypothesis      prob\n",
       "0         she  0.00000...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.991...</td>\n",
       "      <td>0.403339</td>\n",
       "      <td>0.006167</td>\n",
       "      <td>water</td>\n",
       "      <td>/home/stephan/notebooks/ciwganfiwgan-pytorch/t...</td>\n",
       "      <td>0.006827</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          candidates  \\\n",
       "0       hypothesis      prob\n",
       "174       water  0.9...   \n",
       "\n",
       "                                          simplified  \\\n",
       "0     hypothesis      prob\n",
       "0         she  0.00000...   \n",
       "\n",
       "                                                prob  decoding_prob  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.991...       0.403339   \n",
       "\n",
       "   no_speech_prob best_guess_of_string  \\\n",
       "0        0.006167                water   \n",
       "\n",
       "                                            filename  unk_prob  timeout  \n",
       "0  /home/stephan/notebooks/ciwganfiwgan-pytorch/t...  0.006827    False  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "fast_whisper_recognize_wav(\"/home/stephan/notebooks/ciwganfiwgan-pytorch/temp/500/water_1136bf14-c829-41f7-9e86-52c40a1c6de9.wav\", faster_whisper_model, timit_words, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1b57cee1-f06f-44f2-bb8e-9d9d41756850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'df':       word          prob\n",
       " 0      she  1.125344e-07\n",
       " 1      had  1.125344e-07\n",
       " 2     your  1.125344e-07\n",
       " 3     suit  2.061140e-09\n",
       " 4       in  2.061140e-09\n",
       " 5     dark  1.125344e-07\n",
       " 6   greasy  3.775109e-11\n",
       " 7     wash  6.144170e-06\n",
       " 8    water  9.999932e-01\n",
       " 9      all  1.125344e-07\n",
       " 10    year  1.125344e-07,\n",
       " 'lexical_probs': array([1.12534407e-07, 1.12534407e-07, 1.12534407e-07, 2.06113956e-09,\n",
       "        2.06113956e-09, 1.12534407e-07, 3.77510878e-11, 6.14417043e-06,\n",
       "        9.99993176e-01, 1.12534407e-07, 1.12534407e-07]),\n",
       " 'decoding_prob': 0.48548717139804465,\n",
       " 'no_speech_prob': 0.0803394615650177}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_recognize_wav(\"/home/stephan/notebooks/ciwganfiwgan-pytorch/temp/500/water_1136bf14-c829-41f7-9e86-52c40a1c6de9.wav\", whisper_model, timit_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb75c00-6796-42f9-8d6d-ae6eed23b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_recognize_wav(\"/home/stephan/notebooks/ciwganfiwgan-pytorch/temp/500/water_1136bf14-c829-41f7-9e86-52c40a1c6de9.wav\", whisper_model, timit_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68abc3f7-321a-408f-9737-c65bf4d344d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_recognize_wav(\"/home/stephan/notebooks/ciwganfiwgan-pytorch/temp/500/water_426cd806-b512-4aba-a2bd-7c358da4103c.wav\", model, timit_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979c5fac-9af7-4537-a48a-b1a14163baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should not be recognizable\n",
    "test = whisper_recognize_wav(\"/home/stephan/notebooks/ciwganfiwgan-pytorch/temp/500/water_42d73bda-64c9-4867-95bc-d78f7a4c1ef2.wav\", model, timit_words)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5468152-3b55-422e-8386-50c97c58d553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should not be recognizable\n",
    "test = whisper_recognize_wav(\"/home/stephan/notebooks/ciwganfiwgan-pytorch/temp/500/water_733049a5-ec87-40c3-af46-66085a06449e.wav\", model, timit_words)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bb14ec40-d6bb-4bdf-b35b-ec68bc575341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.65 s, sys: 4.14 s, total: 5.79 s\n",
      "Wall time: 532 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidates</th>\n",
       "      <th>simplified</th>\n",
       "      <th>prob</th>\n",
       "      <th>decoding_prob</th>\n",
       "      <th>no_speech_prob</th>\n",
       "      <th>best_guess_of_string</th>\n",
       "      <th>filename</th>\n",
       "      <th>unk_prob</th>\n",
       "      <th>timeout</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>/home/stephan/notebooks/ciwganfiwgan-pytorch/t...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  candidates simplified                                               prob  \\\n",
       "0       None       None  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "  decoding_prob no_speech_prob best_guess_of_string  \\\n",
       "0          None           None                 None   \n",
       "\n",
       "                                            filename unk_prob  timeout  \n",
       "0  /home/stephan/notebooks/ciwganfiwgan-pytorch/t...     None     True  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# should not be recognizable\n",
    "\n",
    "fast_whisper_recognize_wav(\"/home/stephan/notebooks/ciwganfiwgan-pytorch/temp/500/water_733049a5-ec87-40c3-af46-66085a06449e.wav\", faster_whisper_model, timit_words, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b364e4-0740-4fbd-9d80-b1b31e4159bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = whisper_recognize_wav(\"/home/stephan/notebooks/ciwganfiwgan-pytorch/temp/500/water_52a51577-7ef1-4651-a47d-730e9f4c5474.wav\", model, timit_words)\n",
    "print(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "15fea447-8153-475e-b497-3b69e868a6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:\n",
      "0.28522615152731634\n",
      "Score by word\n",
      "remapped_word\n",
      "all       0.246392\n",
      "dark      0.496618\n",
      "greasy    0.791777\n",
      "had       0.000000\n",
      "in        0.000000\n",
      "she       0.000000\n",
      "suit      0.104378\n",
      "wash      0.236777\n",
      "water     0.694357\n",
      "year      0.432365\n",
      "your      0.134823\n",
      "Name: p_correct, dtype: float64\n",
      "CPU times: user 5min 49s, sys: 14min 47s, total: 20min 36s\n",
      "Wall time: 1min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = whisper_recognize_wavs('q2_dev_data', faster_whisper_model, reverse_remapping, vocab, timit_words, n = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "919d4b32-c118-4535-9df1-da7ce9227a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8564859570880302\n",
      "(53, 12)\n"
     ]
    }
   ],
   "source": [
    "df = test['df']\n",
    "selected_df = df.loc[(df.decoding_prob > .1) & (df.no_speech_prob < .1) & (df.unk_prob <.15) & ~df.timeout]\n",
    "print(np.mean(selected_df.p_correct))\n",
    "print(selected_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba61a3ef-7cc6-4c9f-8083-f7e056b38199",
   "metadata": {},
   "outputs": [],
   "source": [
    ".85\n",
    "(53/220 in 101 seconds)\n",
    "\n",
    ".82\n",
    "(54 in 106 seconds\n",
    "\n",
    "# beat \n",
    "#0.8419184657183111\n",
    "#(54, 11) in 120 seconds\n",
    "\n",
    "Beat \n",
    "0.8800743171340705\n",
    "(45, 11) in 168 seconds\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1ff9ce4d-e8c3-4b16-b35f-a0ac140e568c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3095238095238095"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "220 / 168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "300b9e52-7645-47e6-bd42-131620e0a0bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7741935483870968"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "220 / 124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f655fc16-9326-4bb8-9339-196fe85b3c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".4* 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fee386b8-90f8-48e9-b063-139b5d84e7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>word</th>\n",
       "      <th>remapped_word</th>\n",
       "      <th>candidates</th>\n",
       "      <th>simplified</th>\n",
       "      <th>prob</th>\n",
       "      <th>decoding_prob</th>\n",
       "      <th>no_speech_prob</th>\n",
       "      <th>best_guess_of_string</th>\n",
       "      <th>unk_prob</th>\n",
       "      <th>p_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>q2_dev_data/in_28b8473e-9c59-4e80-ba4b-fedefa8...</td>\n",
       "      <td>in</td>\n",
       "      <td>had</td>\n",
       "      <td>hypothesis      prob\n",
       "60         she  0.806...</td>\n",
       "      <td>hypothesis      prob\n",
       "0         she  0.80641...</td>\n",
       "      <td>[0.806414031381023, 0.0, 0.0, 0.0, 0.0, 0.0, 0...</td>\n",
       "      <td>0.527169</td>\n",
       "      <td>0.002693</td>\n",
       "      <td>she</td>\n",
       "      <td>0.187596</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>q2_dev_data/in_cb5c374e-aa2e-4ddf-8baf-2a703a9...</td>\n",
       "      <td>in</td>\n",
       "      <td>had</td>\n",
       "      <td>hypothesis      prob\n",
       "60         she  0.806...</td>\n",
       "      <td>hypothesis      prob\n",
       "0         she  0.80641...</td>\n",
       "      <td>[0.806414031381023, 0.0, 0.0, 0.0, 0.0, 0.0, 0...</td>\n",
       "      <td>0.547741</td>\n",
       "      <td>0.060974</td>\n",
       "      <td>she</td>\n",
       "      <td>0.187596</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>q2_dev_data/in_3738b936-8a44-4798-995b-2a71657...</td>\n",
       "      <td>in</td>\n",
       "      <td>had</td>\n",
       "      <td>hypothesis      prob\n",
       "60         she  0.806...</td>\n",
       "      <td>hypothesis      prob\n",
       "0         she  0.80641...</td>\n",
       "      <td>[0.806414031381023, 0.0, 0.0, 0.0, 0.0, 0.0, 0...</td>\n",
       "      <td>0.512149</td>\n",
       "      <td>0.012817</td>\n",
       "      <td>she</td>\n",
       "      <td>0.187596</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>q2_dev_data/in_db0a37fd-a955-4b8f-9940-1adc4ec...</td>\n",
       "      <td>in</td>\n",
       "      <td>had</td>\n",
       "      <td>hypothesis      prob\n",
       "60         she  0.806...</td>\n",
       "      <td>hypothesis      prob\n",
       "0         she  0.80641...</td>\n",
       "      <td>[0.806414031381023, 0.0, 0.0, 0.0, 0.0, 0.0, 0...</td>\n",
       "      <td>0.555281</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>she</td>\n",
       "      <td>0.187596</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>q2_dev_data/in_777e3e8b-aab5-4a2c-9a8a-98c5519...</td>\n",
       "      <td>in</td>\n",
       "      <td>had</td>\n",
       "      <td>hypothesis      prob\n",
       "1634       suit  0.5...</td>\n",
       "      <td>hypothesis      prob\n",
       "0         she  0.00000...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.5218881956128997, 0.0, 0.0, ...</td>\n",
       "      <td>0.570005</td>\n",
       "      <td>0.002935</td>\n",
       "      <td>suit</td>\n",
       "      <td>0.429889</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>q2_dev_data/in_2d4e33b6-b638-450e-98de-44833d0...</td>\n",
       "      <td>in</td>\n",
       "      <td>had</td>\n",
       "      <td>hypothesis      prob\n",
       "94          say  0.2...</td>\n",
       "      <td>hypothesis      prob\n",
       "0         she  0.00000...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.426081</td>\n",
       "      <td>0.102295</td>\n",
       "      <td>gay</td>\n",
       "      <td>0.755745</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>q2_dev_data/in_7eb430c1-30a7-4a2b-97f0-29f823d...</td>\n",
       "      <td>in</td>\n",
       "      <td>had</td>\n",
       "      <td>hypothesis      prob\n",
       "60         she  0.806...</td>\n",
       "      <td>hypothesis      prob\n",
       "0         she  0.80641...</td>\n",
       "      <td>[0.806414031381023, 0.0, 0.0, 0.0, 0.0, 0.0, 0...</td>\n",
       "      <td>0.465407</td>\n",
       "      <td>0.006851</td>\n",
       "      <td>she</td>\n",
       "      <td>0.187596</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>q2_dev_data/in_aa59d685-b8fe-4ce4-9ae1-5ff71b7...</td>\n",
       "      <td>in</td>\n",
       "      <td>had</td>\n",
       "      <td>hypothesis      prob\n",
       "60         she  0.806...</td>\n",
       "      <td>hypothesis      prob\n",
       "0         she  0.80641...</td>\n",
       "      <td>[0.806414031381023, 0.0, 0.0, 0.0, 0.0, 0.0, 0...</td>\n",
       "      <td>0.584892</td>\n",
       "      <td>0.011597</td>\n",
       "      <td>she</td>\n",
       "      <td>0.187596</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>q2_dev_data/in_7a691732-286d-4714-a89b-76fd5b9...</td>\n",
       "      <td>in</td>\n",
       "      <td>had</td>\n",
       "      <td>hypothesis      prob\n",
       "60         she  0.806...</td>\n",
       "      <td>hypothesis      prob\n",
       "0         she  0.80641...</td>\n",
       "      <td>[0.806414031381023, 0.0, 0.0, 0.0, 0.0, 0.0, 0...</td>\n",
       "      <td>0.584892</td>\n",
       "      <td>0.025955</td>\n",
       "      <td>she</td>\n",
       "      <td>0.187596</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>q2_dev_data/in_058fee18-e5db-484b-87c9-cd83274...</td>\n",
       "      <td>in</td>\n",
       "      <td>had</td>\n",
       "      <td>hypothesis      prob\n",
       "60         she  0.806...</td>\n",
       "      <td>hypothesis      prob\n",
       "0         she  0.80641...</td>\n",
       "      <td>[0.806414031381023, 0.0, 0.0, 0.0, 0.0, 0.0, 0...</td>\n",
       "      <td>0.563585</td>\n",
       "      <td>0.002453</td>\n",
       "      <td>she</td>\n",
       "      <td>0.187596</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>q2_dev_data/in_5ebbde31-9d33-4ab0-9201-83f84cb...</td>\n",
       "      <td>in</td>\n",
       "      <td>had</td>\n",
       "      <td>hypothesis      prob\n",
       "60         she  0.806...</td>\n",
       "      <td>hypothesis      prob\n",
       "0         she  0.80641...</td>\n",
       "      <td>[0.806414031381023, 0.0, 0.0, 0.0, 0.0, 0.0, 0...</td>\n",
       "      <td>0.531719</td>\n",
       "      <td>0.007462</td>\n",
       "      <td>she</td>\n",
       "      <td>0.187596</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>q2_dev_data/in_143be0fd-ecd9-4ecc-841f-bf6b0d4...</td>\n",
       "      <td>in</td>\n",
       "      <td>had</td>\n",
       "      <td>hypothesis      prob\n",
       "60         she  0.806...</td>\n",
       "      <td>hypothesis      prob\n",
       "0         she  0.80641...</td>\n",
       "      <td>[0.806414031381023, 0.0, 0.0, 0.0, 0.0, 0.0, 0...</td>\n",
       "      <td>0.575599</td>\n",
       "      <td>0.006145</td>\n",
       "      <td>she</td>\n",
       "      <td>0.187596</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>q2_dev_data/in_f933bb1a-e5ad-40eb-a3b6-78ea680...</td>\n",
       "      <td>in</td>\n",
       "      <td>had</td>\n",
       "      <td>hypothesis      prob\n",
       "60         she  0.806...</td>\n",
       "      <td>hypothesis      prob\n",
       "0         she  0.80641...</td>\n",
       "      <td>[0.806414031381023, 0.0, 0.0, 0.0, 0.0, 0.0, 0...</td>\n",
       "      <td>0.451793</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>she</td>\n",
       "      <td>0.187596</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>q2_dev_data/in_be3efe72-9f7e-4b9d-8432-16719ca...</td>\n",
       "      <td>in</td>\n",
       "      <td>had</td>\n",
       "      <td>hypothesis      prob\n",
       "60         she  0.806...</td>\n",
       "      <td>hypothesis      prob\n",
       "0         she  0.80641...</td>\n",
       "      <td>[0.806414031381023, 0.0, 0.0, 0.0, 0.0, 0.0, 0...</td>\n",
       "      <td>0.488314</td>\n",
       "      <td>0.063232</td>\n",
       "      <td>she</td>\n",
       "      <td>0.187596</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>q2_dev_data/in_460de3b0-bf47-4fc2-9a97-c1c41d0...</td>\n",
       "      <td>in</td>\n",
       "      <td>had</td>\n",
       "      <td>hypothesis      prob\n",
       "1634       suit  0.5...</td>\n",
       "      <td>hypothesis      prob\n",
       "0         she  0.00000...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.5218881956128997, 0.0, 0.0, ...</td>\n",
       "      <td>0.443054</td>\n",
       "      <td>0.001714</td>\n",
       "      <td>suit</td>\n",
       "      <td>0.429889</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             filename word remapped_word  \\\n",
       "80  q2_dev_data/in_28b8473e-9c59-4e80-ba4b-fedefa8...   in           had   \n",
       "81  q2_dev_data/in_cb5c374e-aa2e-4ddf-8baf-2a703a9...   in           had   \n",
       "82  q2_dev_data/in_3738b936-8a44-4798-995b-2a71657...   in           had   \n",
       "83  q2_dev_data/in_db0a37fd-a955-4b8f-9940-1adc4ec...   in           had   \n",
       "86  q2_dev_data/in_777e3e8b-aab5-4a2c-9a8a-98c5519...   in           had   \n",
       "88  q2_dev_data/in_2d4e33b6-b638-450e-98de-44833d0...   in           had   \n",
       "89  q2_dev_data/in_7eb430c1-30a7-4a2b-97f0-29f823d...   in           had   \n",
       "90  q2_dev_data/in_aa59d685-b8fe-4ce4-9ae1-5ff71b7...   in           had   \n",
       "92  q2_dev_data/in_7a691732-286d-4714-a89b-76fd5b9...   in           had   \n",
       "93  q2_dev_data/in_058fee18-e5db-484b-87c9-cd83274...   in           had   \n",
       "94  q2_dev_data/in_5ebbde31-9d33-4ab0-9201-83f84cb...   in           had   \n",
       "95  q2_dev_data/in_143be0fd-ecd9-4ecc-841f-bf6b0d4...   in           had   \n",
       "96  q2_dev_data/in_f933bb1a-e5ad-40eb-a3b6-78ea680...   in           had   \n",
       "97  q2_dev_data/in_be3efe72-9f7e-4b9d-8432-16719ca...   in           had   \n",
       "98  q2_dev_data/in_460de3b0-bf47-4fc2-9a97-c1c41d0...   in           had   \n",
       "\n",
       "                                           candidates  \\\n",
       "80      hypothesis      prob\n",
       "60         she  0.806...   \n",
       "81      hypothesis      prob\n",
       "60         she  0.806...   \n",
       "82      hypothesis      prob\n",
       "60         she  0.806...   \n",
       "83      hypothesis      prob\n",
       "60         she  0.806...   \n",
       "86       hypothesis      prob\n",
       "1634       suit  0.5...   \n",
       "88       hypothesis      prob\n",
       "94          say  0.2...   \n",
       "89      hypothesis      prob\n",
       "60         she  0.806...   \n",
       "90      hypothesis      prob\n",
       "60         she  0.806...   \n",
       "92      hypothesis      prob\n",
       "60         she  0.806...   \n",
       "93      hypothesis      prob\n",
       "60         she  0.806...   \n",
       "94      hypothesis      prob\n",
       "60         she  0.806...   \n",
       "95      hypothesis      prob\n",
       "60         she  0.806...   \n",
       "96      hypothesis      prob\n",
       "60         she  0.806...   \n",
       "97      hypothesis      prob\n",
       "60         she  0.806...   \n",
       "98       hypothesis      prob\n",
       "1634       suit  0.5...   \n",
       "\n",
       "                                           simplified  \\\n",
       "80     hypothesis      prob\n",
       "0         she  0.80641...   \n",
       "81     hypothesis      prob\n",
       "0         she  0.80641...   \n",
       "82     hypothesis      prob\n",
       "0         she  0.80641...   \n",
       "83     hypothesis      prob\n",
       "0         she  0.80641...   \n",
       "86     hypothesis      prob\n",
       "0         she  0.00000...   \n",
       "88     hypothesis      prob\n",
       "0         she  0.00000...   \n",
       "89     hypothesis      prob\n",
       "0         she  0.80641...   \n",
       "90     hypothesis      prob\n",
       "0         she  0.80641...   \n",
       "92     hypothesis      prob\n",
       "0         she  0.80641...   \n",
       "93     hypothesis      prob\n",
       "0         she  0.80641...   \n",
       "94     hypothesis      prob\n",
       "0         she  0.80641...   \n",
       "95     hypothesis      prob\n",
       "0         she  0.80641...   \n",
       "96     hypothesis      prob\n",
       "0         she  0.80641...   \n",
       "97     hypothesis      prob\n",
       "0         she  0.80641...   \n",
       "98     hypothesis      prob\n",
       "0         she  0.00000...   \n",
       "\n",
       "                                                 prob  decoding_prob  \\\n",
       "80  [0.806414031381023, 0.0, 0.0, 0.0, 0.0, 0.0, 0...       0.527169   \n",
       "81  [0.806414031381023, 0.0, 0.0, 0.0, 0.0, 0.0, 0...       0.547741   \n",
       "82  [0.806414031381023, 0.0, 0.0, 0.0, 0.0, 0.0, 0...       0.512149   \n",
       "83  [0.806414031381023, 0.0, 0.0, 0.0, 0.0, 0.0, 0...       0.555281   \n",
       "86  [0.0, 0.0, 0.0, 0.5218881956128997, 0.0, 0.0, ...       0.570005   \n",
       "88  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       0.426081   \n",
       "89  [0.806414031381023, 0.0, 0.0, 0.0, 0.0, 0.0, 0...       0.465407   \n",
       "90  [0.806414031381023, 0.0, 0.0, 0.0, 0.0, 0.0, 0...       0.584892   \n",
       "92  [0.806414031381023, 0.0, 0.0, 0.0, 0.0, 0.0, 0...       0.584892   \n",
       "93  [0.806414031381023, 0.0, 0.0, 0.0, 0.0, 0.0, 0...       0.563585   \n",
       "94  [0.806414031381023, 0.0, 0.0, 0.0, 0.0, 0.0, 0...       0.531719   \n",
       "95  [0.806414031381023, 0.0, 0.0, 0.0, 0.0, 0.0, 0...       0.575599   \n",
       "96  [0.806414031381023, 0.0, 0.0, 0.0, 0.0, 0.0, 0...       0.451793   \n",
       "97  [0.806414031381023, 0.0, 0.0, 0.0, 0.0, 0.0, 0...       0.488314   \n",
       "98  [0.0, 0.0, 0.0, 0.5218881956128997, 0.0, 0.0, ...       0.443054   \n",
       "\n",
       "    no_speech_prob best_guess_of_string  unk_prob  p_correct  \n",
       "80        0.002693                  she  0.187596        0.0  \n",
       "81        0.060974                  she  0.187596        0.0  \n",
       "82        0.012817                  she  0.187596        0.0  \n",
       "83        0.003538                  she  0.187596        0.0  \n",
       "86        0.002935                 suit  0.429889        0.0  \n",
       "88        0.102295                  gay  0.755745        0.0  \n",
       "89        0.006851                  she  0.187596        0.0  \n",
       "90        0.011597                  she  0.187596        0.0  \n",
       "92        0.025955                  she  0.187596        0.0  \n",
       "93        0.002453                  she  0.187596        0.0  \n",
       "94        0.007462                  she  0.187596        0.0  \n",
       "95        0.006145                  she  0.187596        0.0  \n",
       "96        0.002100                  she  0.187596        0.0  \n",
       "97        0.063232                  she  0.187596        0.0  \n",
       "98        0.001714                 suit  0.429889        0.0  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_df.loc[df.remapped_word == 'had']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94e2bf1-74e5-449c-b8ea-8e3ecd6a2576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "test_audio_filename = selected_df.loc[df.remapped_word == 'she'].iloc[0].filename\n",
    "signal, sample_rate = librosa.load(test_audio_filename, sr=None)\n",
    "\n",
    "# display audio player for the signal\n",
    "display(Audio(data=signal, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb181d-5ceb-4d52-b3c5-bd86ee4ab06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build out the rest of the Bayesian speech recognizer, and map it to UNK\n",
    "if decoding prob is too low, not a word\n",
    "if not_speech, not a work\n",
    "if UNK is high, outside of this set of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dda06d9-90c0-4f42-8400-cc2a10ec665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "# [X] Fix the mapping\n",
    "# [X] make it easy to output candidates here for inspection\n",
    "# [X] train a new LM that upweights these words\n",
    "# [ ] grid search alpha and beta -- how to get the most\n",
    "# [ ] how to handle UNK -- what if we don't? QQ network tries to imitate \n",
    "\n",
    "# [ ] get higher perfomrance on \"greasy\" -- what sorts of errors do we see here?\n",
    "# 1. greasy_16b6aca2-e108-44b7-8b55-7f23a6a60eec_candidates.csv is pretty high\n",
    "# inspect \n",
    "# [ ] dumb heuristic Whisper\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdl-asr-env",
   "language": "python",
   "name": "cdl-asr-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
